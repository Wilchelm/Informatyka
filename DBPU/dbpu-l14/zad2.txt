Istnieje szereg problemów związanych z bezpieczeństwem kodowania, gdy producent i konsument nie zgadzają się co do używanego kodowania lub sposobu implementacji danego kodowania. Na przykład w 2011 r. Zgłoszono atak, w którym bajt wiodący Shift_JIS 0x82 został użyty do „zamaskowania” bajtu szlaku 0x22 w zasobie JSON, którego atakujący mógł kontrolować niektóre pola. Producent nie widział problemu, mimo że jest to nielegalna kombinacja bajtów. Konsument zdekodował go jako pojedynczy U + FFFD i dlatego zmienił ogólną interpretację, ponieważ U + 0022 jest ważnym ogranicznikiem. Dekodery kodowania, które używają wielu bajtów dla wartości skalarnych, wymagają obecnie, aby w przypadku niedozwolonej kombinacji bajtów wartość skalarna z zakresu od U + 0000 do U + 007F włącznie nie mogła być „zamaskowana”. Dla wyżej wymienionej sekwencji wyjściem byłoby U + FFFD U + 0022.

Jest to większy problem w przypadku kodowania, które odwzorowuje wszystko, co jest bajtem ASCII, na coś, co nie jest kodem ASCII, gdy nie ma bajtu wiodącego. Są to kodowania „niezgodne z ASCII” i inne niż ISO-2022-JP, UTF-16BE i UTF-16LE, które niestety są wymagane ze względu na wdrożoną treść, nie są obsługiwane. (Trwa badanie, czy więcej etykiet innych takich kodowań może zostać zamapowanych na kodowanie zastępcze, a nie na nieznane awarie kodowania). Przykładowym atakiem jest wstrzyknięcie starannie spreparowanej treści do zasobu, a następnie zachęcenie użytkownika do zastąpienia kodowania, co powoduje na przykład wykonanie skryptu.

Kodery używane przez adresy URL znajdujące się w HTML i funkcji formularza HTML mogą również powodować niewielką utratę informacji, gdy stosowane jest kodowanie, które nie może reprezentować wszystkich wartości skalarnych. Na przykład. gdy zasób korzysta z kodowania Windows-1252, serwer nie będzie w stanie odróżnić użytkownika końcowego wprowadzającego „💩” i „& # 128169;” w formę.

Przedstawione tutaj problemy znikają, gdy używa się wyłącznie UTF-8, co jest jednym z wielu powodów, które są obecnie obowiązkowym kodowaniem wszystkich rzeczy.

Byte order mark

Wiele programów Windows (w tym starsze wersje Notatnika Windows) dodaje bajty 0xEF, 0xBB, 0xBF na początku każdego dokumentu zapisanego jako UTF-8. Jest to kodowanie UTF-8 znaku kolejności bajtów Unicode (BOM) i jest powszechnie określane jako BOM UTF-8, nawet jeśli kolejność bajtów nie ma znaczenia dla UTF-8. Chociaż tekst ASCII zakodowany przy użyciu UTF-8 zwykle jest wstecznie kompatybilny z ASCII, nie jest to prawdą, gdy zalecenia Standardu Unicode są ignorowane i dodawana jest BOM. Oprogramowanie inne niż UTF-8 może wyświetlać BOM jako trzy śmieciowe znaki, np. „ï» ¿”w oprogramowaniu interpretującym dokument jako ISO 8859-1 lub Windows-1252 oraz„ ∩╗┐ ”, jeśli jest interpretowany jako strona kodowa 437. Jest to przykład mojibake, wynik zniekształconego tekstu podczas dekodowania tekstu przy użyciu niezamierzone kodowanie znaków.

Standard Unicode nie wymaga ani nie zaleca używania BOM dla UTF-8, ale ostrzega, że ​​można go napotkać na początku pliku transkodowanego z innego kodowania. Obecność BOM UTF-8 może powodować problemy z istniejącym oprogramowaniem obsługującym UTF-8, na przykład:

    Parsery języka programowania, które nie zostały wyraźnie zaprojektowane dla UTF-8, mogą często obsługiwać UTF-8 w stałych ciągów i komentarzach, ale nie mogą analizować BOM UTF-8 na początku pliku.
    Programy, które identyfikują typy plików za pomocą wiodących znaków, mogą nie zidentyfikować pliku, jeśli BOM UTF-8 jest obecny, nawet jeśli użytkownik pliku może obsłużyć BOM. Przykładem jest uniksowa składnia shebang, inna to Internet Explorer, który będzie renderował strony w trybie standardów tylko wtedy, gdy zacznie się od deklaracji typu dokumentu.
    Programy, które wstawiają informacje na początku pliku, przestaną używać BOM do identyfikacji UTF-8 (jednym z przykładów są przeglądarki offline, które dodają źródłowy adres URL na początku pliku).

Niemniej jednak niektóre programy, takie jak edytory tekstu, odmawiają poprawnego wyświetlania lub interpretacji UTF-8, chyba że tekst zaczyna się znakiem bajtu lub zawiera tylko ASCII.

Konsorcjum Unicode rozpoznało problem z wieloma reprezentacjami i zmieniło Standard Unicode, aby uniemożliwić wielokrotne reprezentacje tego samego punktu kodowego przy UTF-8. Sprostowanie UTF-8 wymienia nowo ograniczony zakres UTF-8 (patrz referencje). Wiele obecnych aplikacji mogło nie zostać zmienionych w celu przestrzegania tej zasady. Sprawdź, czy aplikacja jest zgodna z najnowszą specyfikacją kodowania UTF-8. Zwróć szczególną uwagę na filtrowanie nielegalnych znaków.

Dokładna odpowiedź wymagana od dekodera UTF-8 na nieprawidłowe wejście nie jest jednolicie zdefiniowana przez standardy. Zasadniczo istnieje kilka sposobów zachowania dekodera UTF-8 w przypadku nieprawidłowej sekwencji bajtów:

    1. Wpisz znak zastępczy (np. „?”, „”).
    2. Zignoruj ​​bajty.
    3. Interpretuj bajty zgodnie z innym kodowaniem znaków (często mapa znaków ISO-8859-1).
    4. Nie zauważaj i nie dekoduj tak, jakby bajty były jakimś podobnym bitem UTF-8.
    5. Zatrzymaj dekodowanie i zgłoś błąd (prawdopodobnie dając dzwoniącemu opcję kontynuacji).

Dekoder może zachowywać się w różny sposób dla różnych typów nieprawidłowych danych wejściowych.

RFC 3629 wymaga jedynie, aby dekodery UTF-8 nie dekodowały „długich sekwencji” (gdzie znak jest zakodowany w większej liczbie bajtów niż jest to konieczne, ale nadal przestrzega powyższych formularzy). Standard Unicode wymaga dekodera zgodnego z Unicode, aby „... traktować każdą źle sformułowaną sekwencję jednostek kodu jako warunek błędu. Gwarantuje to, że nie będzie ona interpretować ani emitować nieprawidłowej sekwencji jednostek kodu”.

Przedłużone formularze są jednym z najbardziej kłopotliwych typów danych UTF-8. Obecny RFC mówi, że nie należy ich dekodować, ale starsze specyfikacje dla UTF-8 dały tylko ostrzeżenie i wiele prostszych dekoderów z przyjemnością je zdekoduje. Zbyt długie formularze zostały wykorzystane do ominięcia sprawdzania poprawności zabezpieczeń w głośnych produktach, w tym na serwerze internetowym Microsoft IIS. Dlatego należy bardzo uważać, aby uniknąć problemów związanych z bezpieczeństwem, jeśli sprawdzanie poprawności odbywa się przed konwersją z UTF-8, i ogólnie znacznie łatwiej jest obsługiwać zbyt długie formularze przed wykonaniem jakiejkolwiek weryfikacji danych wejściowych.

Aby zachować bezpieczeństwo w przypadku nieprawidłowych danych wejściowych, istnieją dwie opcje. Pierwszym z nich jest zdekodowanie UTF-8 przed wykonaniem jakichkolwiek sprawdzeń poprawności danych wejściowych. Drugim jest użycie dekodera, który w przypadku nieprawidłowego wprowadzenia zwraca błąd lub tekst, który aplikacja uważa za nieszkodliwy. Inną możliwością jest całkowite uniknięcie konwersji z UTF-8, ale zależy to od innego oprogramowania, które dane są przekazywane do bezpiecznego przetwarzania nieprawidłowych danych.

Innym zagadnieniem jest odzyskiwanie po błędzie. Aby zagwarantować prawidłowe odzyskiwanie po uszkodzonych lub utraconych bajtach, dekodery muszą być w stanie rozpoznać różnicę między bajtami wiodącymi i bajtowymi, a nie tylko zakładać, że bajty będą typu dozwolonego na ich pozycji.
Ze względów bezpieczeństwa dekoder UTF-8 nie może akceptować sekwencji UTF-8 dłuższych niż jest to konieczne do zakodowania znaku. Jeśli używasz analizatora składni do dekodowania kodowania UTF-8, upewnij się, że analizator składni filtruje niepoprawne znaki UTF-8 (nieprawidłowe formularze lub formularze zbyt długie).
Poszukaj zbyt długich sekwencji UTF-8, zaczynając od złośliwego wzorca. Możesz także użyć testu warunków skrajnych dekodera UTF-8, aby przetestować parser UTF-8 (zobacz Markus Kuhn UTF-8 i FAQ dotyczące Unicode w części referencyjnej)
Załóżmy, że wszystkie dane wejściowe są złośliwe. Utwórz białą listę, która definiuje wszystkie prawidłowe dane wejściowe do systemu oprogramowania na podstawie specyfikacji wymagań. Dane, które nie pasują do białej listy, nie powinny być dopuszczane do systemu. Przetestuj proces dekodowania pod kątem złośliwych danych wejściowych.

Wizualne spoofing: powiedz, że masz forum z użytkownikiem o nazwie "admin", któremu wszyscy ufają. Ktoś inny może zarejestrować konto użytkownika o nazwie "аdmin" (pierwsza litera to cyrylica a) i oszukać innych, by myśleli, że to administrator strony. Jest to głównie technika inżynierii społecznej: jest mało prawdopodobne, aby jakiekolwiek oprogramowanie pomieszało użytkowników. (Ten konkretny przykład można częściowo rozwiązać, dodając do strony specjalne formatowanie lub styl w pobliżu nazwy administratora, dzięki czemu nazwy profilu są linkami do stron profilu, które pokazują historię aktywności użytkownika i datę dołączenia itp., Dzięki czemu użytkownicy mogą identyfikować innych na różne sposoby oprócz ich widocznej możliwej do wymyślenia nazwy. Jest to bardziej ogólny problem, który nie ogranicza się wyłącznie do obsługi Unicode: użytkownicy mogą również nazywać siebie innymi mylącymi nazwami, takimi jak "<site> Support", "admin " ze spacją, "admim" itp. )

Normalizacja: niektóre znaki, takie jak „ö”, mogą być reprezentowane na wiele sposobów. Może to być albo pojedynczy znak U + 00F6 (LATIN MAŁY LITER O Z DIAERESISEM), albo dwa znaki U + 0061 U + 0308 (LATIN SMALL LETTER O + COMBINING DIAERESIS). Normalizacja to proces przekształcania całego tekstu w formę złożoną lub rozłożoną. Jeśli konsekwentnie nigdy nie korzystasz z normalizacji lub zawsze korzystasz z normalizacji, nie napotkasz problemów. Jeśli jednak tak się dzieje, możesz mieć problemy z bezpieczeństwem:

Na przykład OS X normalizuje Unicode w nazwach plików. Załóżmy, że masz witrynę internetową bez kodu normalizacyjnego uruchomioną na serwerze OS X, gdzie za każdym razem, gdy użytkownik się rejestruje, tworzony jest plik z jego nazwą i korzystasz z bazy danych bez normalizacji, aby śledzić nazwy użytkowników, które zostały już zarejestrowane w kolejności aby zapobiec ponownej rejestracji nazw. Jeśli masz użytkownika o nazwie „foö” (używając U + 00F6), ktoś inny może zarejestrować konto o nazwie „foö” (U + 0061 U + 0308), a witryna na to pozwoli, ale nadpisze plik utworzony przez pierwszy użytkownik „foö”. Aby rozwiązać ten problem, musisz albo konsekwentnie normalizować aplikację w całej aplikacji, albo sprawdzać kolizje za każdym razem, gdy przekroczysz granicę, która normalizuje się inaczej (gdy użytkownik się zarejestruje i musisz utworzyć plik dla nich , otwórz plik w trybie wyłączności, aby nie powiódł się, jeśli plik już istnieje, i możesz zablokować rejestrację nowego użytkownika).


